import phoenix as px
import chromadb
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from opentelemetry import trace as otel_trace
from phoenix.otel import register

MODEL_NAME = "intfloat/multilingual-e5-small"
CHROMA_HOST = "localhost"
CHROMA_PORT = 8000
COLLECTION_NAME = "thesis_collection"

# ==========================================
# 1. å•Ÿå‹•å…¨åœ°ç«¯ç›£æ§ (Arize Phoenix)
# ==========================================
# å•Ÿå‹•å¾Œå¯åœ¨ç€è¦½å™¨é–‹å•Ÿ http://localhost:6006 æŸ¥çœ‹è¿½è¹¤ç´€éŒ„
session = px.launch_app()
# æ‰‹å‹•å»ºç«‹è¿½è¹¤å°å‘
tracer_provider = register(
    project_name="my-thesis-app", # ç‚ºæ‚¨çš„è«–æ–‡å°ˆæ¡ˆå‘½å
    auto_instrument=True          # è‡ªå‹•åµæ¸¬ä¸¦æ›è¼‰ç’°å¢ƒä¸­çš„ OI å¥—ä»¶
)

print(f"âœ… Phoenix ç›£æ§ä»‹é¢å·²å•Ÿå‹•: {session.url}")
print(f"ç›®å‰è¿½è¹¤å™¨ç‹€æ…‹: {otel_trace.get_tracer_provider()}")


# ==========================================
# 2. è¼‰å…¥åœ¨åœ°ç«¯è³‡æº (Embedding & Vector DB)
# ==========================================
print("å€’å…¥å‘é‡è³‡æ–™åº«ä¸­...")
# ä½¿ç”¨èˆ‡ç¬¬ä¸€æ”¯ç¨‹å¼ç›¸åŒçš„ Embedding æ¨¡å‹
embeddings = HuggingFaceEmbeddings(
    model_name=MODEL_NAME,
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

# è¼‰å…¥å·²å­˜åœ¨çš„ Chroma è³‡æ–™åº«
persistent_client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)

vectorstore = Chroma(
    client=persistent_client,
    collection_name=COLLECTION_NAME,
    embedding_function=embeddings
)

# è¨­å®šæª¢ç´¢å™¨ (æ‰¾æœ€ç›¸é—œçš„ 3 å€‹ç‰‡æ®µ)
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# ==========================================
# 3. åˆå§‹åŒ–å¾®èª¿å¾Œçš„ TAIDE æ¨¡å‹ (Ollama)
# ==========================================
llm = ChatOllama(
    model="cwchang/llama3-taide-lx-8b-chat-alpha1:latest", # ç¢ºä¿æ‚¨å·²åŸ·è¡Œ ollama pull taide
    temperature=0.3,
)

# ==========================================
# 4. è¨­è¨ˆ RAG æµç¨‹ (LangChain LCEL)
# ==========================================
# é‡å°è«–æ–‡ç ”ç©¶è¨­è¨ˆçš„ Prompt
template = """ä½ æ˜¯ä¸€ä½ç†Ÿè®€ä¸­æ­£å¤§å­¸æ ¡è¦çš„åŠ©æ‰‹ã€‚è«‹ç¦®è²Œçš„æ ¹æ“šä»¥ä¸‹æä¾›çš„æ–‡ç»å…§å®¹ï¼Œ
ä»¥ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œã€‚è‹¥å…§å®¹ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹èª å¯¦å›ç­”ä¸çŸ¥é“ï¼Œä¸è¦ç·¨é€ äº‹å¯¦ã€‚

æ–‡ç»å…§å®¹ï¼š
{context}

å•é¡Œï¼š{question}

å°ˆæ¥­å›ç­”ï¼š"""

prompt = ChatPromptTemplate.from_template(template)

# å®šç¾© RAG éˆ
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {
        "context": (lambda x: f"query: {x}") | retriever | format_docs, 
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
    | StrOutputParser()
)

# ==========================================
# 5. åŸ·è¡ŒæŸ¥è©¢
# ==========================================
print("\n--- ç³»çµ±æº–å‚™å°±ç·’ ---")
try:
    while True:
        user_query = input("\nè«‹è¼¸å…¥æ‚¨çš„è«–æ–‡ç›¸é—œå•é¡Œ (è¼¸å…¥ 'exit' é›¢é–‹): ")
        if user_query.lower() == 'exit':
            break
        
        print("\næ­£åœ¨æª¢ç´¢ä¸¦ç”Ÿæˆå›ç­”...")
        # åŸ·è¡Œ RAG
        response = rag_chain.invoke(user_query)
        
        print(f"\n[TAIDE å›è¦†]:\n{response}")
        print("\nğŸ’¡ æç¤ºï¼šæ‚¨å¯ä»¥åˆ° Phoenix ä»‹é¢æŸ¥çœ‹æª¢ç´¢åˆ°çš„åŸæ–‡ç‰‡æ®µã€‚")

except KeyboardInterrupt:
    print("\nç¨‹å¼å·²çµæŸ")

# ä¿æŒ Phoenix é‹ä½œç›´åˆ°æ‰‹å‹•é—œé–‰
input("\næŒ‰ä¸‹ Enter éµçµæŸä¸¦é—œé–‰ç›£æ§ä¼ºæœå™¨...")
