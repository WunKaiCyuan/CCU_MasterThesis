import os
import torch
import uuid
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_experimental.text_splitter import SemanticChunker
import chromadb

from core.serializable_mongodb_byte_store import SerializableMongoDBByteStore
from core.document_loader import load_documents

SETTINGS = {
    "MODEL_NAME": "intfloat/multilingual-e5-small",
    "CHROMA_HOST": "localhost",
    "CHROMA_PORT": 8000,
    "CHROMA_COLLECTION": "ccu_rules_semantic_child",
    "MONGODB_URI": "mongodb://admin:UTWi1dCo6jFxNlS0@localhost:27017",
    "MONGODB_DB": "ccu_school_rules",
    "MONGODB_COLLECTION": "ccu_rules_semantic_parent",
    "DATA_DIR": "./data/",
    "PARENT_CHUNK_SIZE": 1000,
    "PARENT_CHUNK_OVERLAP": 0,
    "BREAKPOINT_PERCENTILE": 95 
}

def main():
    print("ğŸš€ é–‹å§‹åŸ·è¡Œ [æ‰‹å‹•èªæ„åˆ‡åˆ† + Parent-Child] å…¥åº«æµç¨‹...")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    embeddings = HuggingFaceEmbeddings(
        model_name=SETTINGS["MODEL_NAME"],
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True}
    )

    client = chromadb.HttpClient(host=SETTINGS["CHROMA_HOST"], port=SETTINGS["CHROMA_PORT"])
    vectorstore = Chroma(
        client=client,
        collection_name=SETTINGS["CHROMA_COLLECTION"],
        embedding_function=embeddings
    )
    
    store = SerializableMongoDBByteStore(
        connection_string=SETTINGS["MONGODB_URI"],
        db_name=SETTINGS["MONGODB_DB"],
        collection_name=SETTINGS["MONGODB_COLLECTION"]
    )

    # 1. åˆå§‹åŒ–åˆ‡åˆ†å™¨
    parent_splitter = RecursiveCharacterTextSplitter(
        chunk_size=SETTINGS["PARENT_CHUNK_SIZE"], 
        chunk_overlap=SETTINGS["PARENT_CHUNK_OVERLAP"],
        separators=["\n?ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¢", "\n?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ã€", "\n\n", "\n", "ã€‚", " "],
        is_separator_regex=True
    )
    
    # é€™è£¡åªç”¨æ–¼è¨ˆç®—ï¼Œä¸ç›´æ¥å‚³çµ¦ retriever
    semantic_chunker = SemanticChunker(
        embeddings,
        breakpoint_threshold_type="percentile",
        breakpoint_threshold_amount=SETTINGS["BREAKPOINT_PERCENTILE"]
    )

    # 2. è¼‰å…¥æ–‡ä»¶
    raw_documents = load_documents(SETTINGS["DATA_DIR"], clean=True)
    
    print("âœ‚ï¸ æ­£åœ¨æ‰‹å‹•è™•ç†èªæ„åˆ‡åˆ†ä¸¦å»ºç«‹ Parent-Child é—œè¯...")
    
    for i, doc in enumerate(raw_documents):
        file_name = os.path.basename(doc.metadata.get("source", "unknown"))
        print(f"[{i+1}/{len(raw_documents)}] è™•ç†æª”æ¡ˆ: {file_name}")
        
        # A. å…ˆåˆ‡å‡º Parent Chunks
        parent_chunks = parent_splitter.split_documents([doc])
        
        for p_chunk in parent_chunks:
            # ç‚ºé€™å€‹ Parent Chunk ç”¢ç”Ÿå”¯ä¸€ ID
            parent_id = str(uuid.uuid4())
            p_chunk.page_content = f"[è³‡æ–™ä¾†æº:{file_name}]\n{p_chunk.page_content.strip()}"
            p_chunk.metadata["file_name"] = file_name
            p_chunk.metadata["doc_id"] = parent_id # ä¾›åƒè€ƒç”¨
            
            # B. å„²å­˜ Parent åˆ° MongoDB
            store.mset([(parent_id, p_chunk)])
            
            # C. ç”¨èªæ„åˆ‡åˆ†åˆ‡å‡º Child Chunks
            child_chunks = semantic_chunker.split_documents([p_chunk])
            
            # D. ç‚º Child åŠ ä¸Š parent_id ä¸¦å­˜å…¥ Chroma
            for c_chunk in child_chunks:
                c_chunk.metadata["parent_id"] = parent_id
            
            vectorstore.add_documents(child_chunks)

    print(f"âœ… å…¥åº«å®Œæˆï¼")
    print(f"ğŸ“Š Chroma (èªæ„å­å‘é‡) ç¸½æ•¸: {vectorstore._collection.count()}")

if __name__ == "__main__":
    main()