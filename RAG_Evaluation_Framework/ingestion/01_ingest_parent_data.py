import os
import torch
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.retrievers import ParentDocumentRetriever
from langchain_text_splitters import RecursiveCharacterTextSplitter
import chromadb

from core.serializable_mongodb_byte_store import SerializableMongoDBByteStore
from core.document_loader import load_documents

# --- ç›´æ¥è¨­å®šåƒæ•¸ (Hardcoded Settings) ---
SETTINGS = {
    "MODEL_NAME": "intfloat/multilingual-e5-small",
    "CHROMA_HOST": "localhost",
    "CHROMA_PORT": 8000,
    "CHROMA_COLLECTION": "ccu_rules",
    "MONGODB_URI": "mongodb://admin:UTWi1dCo6jFxNlS0@localhost:27017",
    "MONGODB_DB": "ccu_school_rules",
    "MONGODB_COLLECTION": "ccu_rules",
    "DATA_DIR": "./data/",
    "PARENT_CHUNK_SIZE": 1000,
    "PARENT_CHUNK_OVERLAP": 0,
    "CHILD_CHUNK_SIZE": 200,
    "CHILD_CHUNK_OVERLAP": 20
}

def main():
    print("ğŸš€ é–‹å§‹åŸ·è¡Œ Parent-Child å…¥åº«æµç¨‹...")

    # 1. åˆå§‹åŒ– Embedding (ä½¿ç”¨ GPU)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ§¬ ä½¿ç”¨è¨­å‚™: {device}")
    
    embeddings = HuggingFaceEmbeddings(
        model_name=SETTINGS["MODEL_NAME"],
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True}
    )

    # 2. é€£æ¥è³‡æ–™åº« (Chroma & MongoDB)
    client = chromadb.HttpClient(host=SETTINGS["CHROMA_HOST"], port=SETTINGS["CHROMA_PORT"])
    vectorstore = Chroma(
        client=client,
        collection_name=SETTINGS["CHROMA_COLLECTION"],
        embedding_function=embeddings
    )
    
    store = SerializableMongoDBByteStore(
        connection_string=SETTINGS["MONGODB_URI"],
        db_name=SETTINGS["MONGODB_DB"],
        collection_name=SETTINGS["MONGODB_COLLECTION"]
    )

    # 3. é…ç½®åˆ‡åˆ†å™¨
    separators = ["\n?ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¢", "\n?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ã€", "\n\n", "\n", "ã€‚", " "]
    
    parent_splitter = RecursiveCharacterTextSplitter(
        chunk_size=SETTINGS["PARENT_CHUNK_SIZE"], 
        chunk_overlap=SETTINGS["PARENT_CHUNK_OVERLAP"],
        separators=separators, 
        is_separator_regex=True
    )
    
    child_splitter = RecursiveCharacterTextSplitter(
        chunk_size=SETTINGS["CHILD_CHUNK_SIZE"], 
        chunk_overlap=SETTINGS["CHILD_CHUNK_OVERLAP"],
        separators=separators, 
        is_separator_regex=True
    )

    # 4. åˆå§‹åŒ– Retriever
    retriever = ParentDocumentRetriever(
        vectorstore=vectorstore,
        docstore=store,
        child_splitter=child_splitter,
    )

    # 5. è¼‰å…¥ä¸¦é è™•ç†æ–‡æª”
    if not os.path.exists(SETTINGS["DATA_DIR"]):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°è³‡æ–™å¤¾ {SETTINGS["DATA_DIR"]}")
        return

    raw_documents = load_documents(SETTINGS["DATA_DIR"], clean=True)
    
    processed_parents = []
    print("âœ‚ï¸ æ­£åœ¨é€²è¡Œçˆ¶æ–‡ä»¶åˆ‡åˆ†èˆ‡ Metadata æ³¨å…¥...")
    for doc in parent_splitter.split_documents(raw_documents):
        # å–å¾—ç´”æª”å
        source_path = doc.metadata.get("source", "unknown")
        file_name = os.path.basename(source_path)
        
        # åŠ ä¸Šä¾†æºæ¨™ç±¤ï¼Œé€™å°å¾ŒçºŒ RAG å›ç­”å¾ˆæœ‰å¹«åŠ©
        doc.page_content = f"[è³‡æ–™ä¾†æº:{file_name}]\n{doc.page_content.strip()}"
        doc.metadata["file_name"] = file_name
        processed_parents.append(doc)

    # 6. åŸ·è¡Œå…¥åº«
    print(f"ğŸ“¦ æ­£åœ¨å¯«å…¥ {len(processed_parents)} å€‹çˆ¶æ–‡ä»¶æ®µè½è‡³ MongoDB èˆ‡ Chroma...")
    # retriever.add_documents æœƒè‡ªå‹•èª¿ç”¨ child_splitter æŠŠçˆ¶æ–‡ä»¶åˆ‡æˆå­æ–‡ä»¶ä¸¦å­˜å…¥ Chroma
    retriever.add_documents(processed_parents)
    
    print(f"âœ… å…¥åº«å®Œæˆï¼")
    print(f"ğŸ“Š Chroma ç›®å‰å…±æœ‰ {vectorstore._collection.count()} å€‹å­å‘é‡ã€‚")

if __name__ == "__main__":
    main()