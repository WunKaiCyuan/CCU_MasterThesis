import json
import os
import time
import google.generativeai as genai
import typing_extensions as typing

# --- 1. é…ç½®èˆ‡æ¨¡å‹è¨­å®š ---

# å®šç¾©å–®å€‹æ–‡æª”çš„çµæ§‹
class CandidateDoc(typing.TypedDict):
    doc_id: str
    file_name: str
    reason: str

# å®šç¾©æ‰¹æ¬¡å›ç­”çš„çµæ§‹
class QuestionResult(typing.TypedDict):
    question_id: int
    suggested_docs: list[CandidateDoc]

GENAI_API_KEY = "API_KEY"
MODEL_NAME = "models/gemini-2.5-flash"
genai.configure(api_key=GENAI_API_KEY)

# åˆå§‹åŒ–æ¨¡å‹ï¼Œå¼·åˆ¶è¦æ±‚å›å‚³ JSON Array[QuestionResult]
model = genai.GenerativeModel(
    model_name=MODEL_NAME,
    generation_config={
        "temperature": 0,
        "response_mime_type": "application/json",
        "response_schema": list[QuestionResult],
    }
)

# --- 2. å·¥å…·å‡½å¼ ---

def load_json(file_path):
    if not os.path.exists(file_path):
        print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_path}")
        return [] if "questions" in file_path else {}
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_json(data, file_path):
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"âœ… æª”æ¡ˆå·²å„²å­˜è‡³: {file_path}")

def batch_llm_suggest(questions_subset, doc_list):
    """å°‡å¤šå€‹å•é¡Œæ‰“åŒ…ï¼Œä¸€æ¬¡æ€§è©¢å• Gemini"""
    
    # å»ºç«‹æ–‡æª”æ¸…å–®æ–‡æœ¬
    doc_context = "\n".join([f"- {d['doc_id']}: {d['file_name']}" for d in doc_list])
    
    # å»ºç«‹å•é¡Œæ¸…å–®æ–‡æœ¬
    questions_context = "\n".join([f"Q{q['id']}: {q['question']}" for q in questions_subset])

    prompt = f"""
ä½ æ˜¯ä¸€ä½æ³•å¾‹æ–‡æª”å°ˆå®¶ã€‚æˆ‘æ­£åœ¨è™•ç†ã€Œä¸­æ­£å¤§å­¸æ ¡è¦ã€çš„ RAG æª¢ç´¢å„ªåŒ–ç ”ç©¶ã€‚
è«‹é‡å°ä»¥ä¸‹ã€Œå•é¡Œæ¸…å–®ã€ä¸­çš„æ¯ä¸€å€‹å•é¡Œï¼Œå¾ã€Œæ–‡æª”æ¸…å–®ã€ä¸­æŒ‘é¸å‡º 1-5 ä»½æœ€å¯èƒ½åŒ…å«ç­”æ¡ˆçš„å€™é¸æ–‡æª”ã€‚

ã€æ–‡æª”æ¸…å–®ã€‘:
{doc_context}

ã€å•é¡Œæ¸…å–®ã€‘:
{questions_context}

ã€è¦æ±‚ã€‘:
1. é‡å°æ¯å€‹å•é¡Œçµ¦äºˆæ¨è–¦ç†ç”± reasonã€‚
2. å¿…é ˆåš´æ ¼æŒ‰ç…§æä¾›çš„ JSON Schema å›å‚³ï¼Œç¢ºä¿ question_id èˆ‡å•é¡Œæ¸…å–®å°æ‡‰ã€‚
"""
    
    try:
        response = model.generate_content(prompt)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å…§å®¹ (è™•ç†å®‰å…¨éæ¿¾)
        if not response.text:
            print("âš ï¸ æ¨¡å‹æœªå›å‚³å…§å®¹ï¼Œå¯èƒ½æ˜¯è¢«å®‰å…¨æ©Ÿåˆ¶æ””æˆªã€‚")
            return []
            
        return json.loads(response.text)
    except Exception as e:
        print(f"âŒ æ‰¹æ¬¡è«‹æ±‚å‡ºéŒ¯: {e}")
        return []

# --- 3. ä¸»ç¨‹å¼æµç¨‹ ---

def main():
    # è¨­å®šæª”æ¡ˆè·¯å¾‘
    INDEX_PATH = "./output/document_index.json"
    QUESTIONS_PATH = "./output/generated_questions.json"
    OUTPUT_PATH = "./output/candidate_mapping.json"

    # è¼‰å…¥è³‡æ–™
    inventory = load_json(INDEX_PATH)
    questions_all = load_json(QUESTIONS_PATH)
    docs = inventory.get("documents", [])

    if not docs or not questions_all:
        print("è³‡æ–™è¼‰å…¥å¤±æ•—ï¼Œè«‹ç¢ºèª JSON æª”æ¡ˆå…§å®¹èˆ‡è·¯å¾‘ã€‚")
        return

    mapped_results = []
    
    # --- æ‰¹æ¬¡è™•ç†è¨­å®š ---
    BATCH_SIZE = 10  # æ¯æ¬¡è™•ç† 10 å€‹å•é¡Œ
    SLEEP_TIME = 15  # æ¯æ¬¡è«‹æ±‚å¾Œä¼‘æ¯ 15 ç§’ä»¥ç¬¦åˆ Free Tier é™åˆ¶ (5 RPM)

    total_questions = len(questions_all)
    print(f"ğŸš€ é–‹å§‹åˆ†æï¼ç¸½è¨ˆ {total_questions} å€‹å•é¡Œï¼Œé è¨ˆåˆ†ç‚º { (total_questions // BATCH_SIZE) + 1 } æ‰¹æ¬¡...")

    for i in range(0, total_questions, BATCH_SIZE):
        subset = questions_all[i : i + BATCH_SIZE]
        current_batch_ids = [q['id'] for q in subset]
        
        print(f"\nğŸ“¦ æ­£åœ¨è™•ç†æ‰¹æ¬¡: Q{current_batch_ids[0]} ~ Q{current_batch_ids[-1]}...")
        
        # å‘¼å« Gemini é€²è¡Œæ‰¹æ¬¡è™•ç†
        batch_results = batch_llm_suggest(subset, docs)
        
        # å°‡ LLM çš„çµæœå°ç…§å›åŸæœ¬çš„å•é¡Œæ¸…å–®
        for q in subset:
            # åœ¨ LLM å›å‚³çš„æ¸…å–®ä¸­å°‹æ‰¾å°æ‡‰çš„ id
            matched_llm_res = next((res for res in batch_results if res.get('question_id') == q['id']), None)
            
            suggested = matched_llm_res.get('suggested_docs', []) if matched_llm_res else []
            
            mapped_results.append({
                "question_id": q["id"],
                "question": q["question"],
                "category": q.get("category", ""),
                "llm_suggested_candidates": suggested,
                "manual_confirmed_doc_ids": [] 
            })
            
        print(f"âœ”ï¸ æ‰¹æ¬¡è™•ç†å®Œæˆ (æˆåŠŸåŒ¹é…: {len([r for r in batch_results])} ç­†)")
        
        # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€è¼ªï¼Œå‰‡é€²å…¥å†·å»
        if i + BATCH_SIZE < total_questions:
            print(f"ğŸ˜´ ç­‰å¾… {SLEEP_TIME} ç§’ä»¥éµå®ˆ API é »ç‡é™åˆ¶...")
            time.sleep(SLEEP_TIME)

    # å„²å­˜æœ€çµ‚çµæœ
    save_json(mapped_results, OUTPUT_PATH)
    print("\nâœ¨ å…¨éƒ¨ä»»å‹™å®Œæˆï¼è«‹æŸ¥çœ‹ output æª”æ¡ˆã€‚")

if __name__ == "__main__":
    main()