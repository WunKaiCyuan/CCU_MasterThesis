import json
import os
import time
import google.generativeai as genai
from datetime import timedelta
from core.document_loader import load_documents
import typing_extensions as typing

# --- 1. å®šç¾© Schema ---
class SummaryResponse(typing.TypedDict):
    summary: str
    doc_name: str
    keywords: list

# --- 2. é…ç½® ---
GENAI_API_KEY = "API_KEY"
MODEL_NAME = "models/gemini-2.5-flash"
genai.configure(api_key=GENAI_API_KEY)

INDEX_PATH = "./output/document_index.json"
DATA_DIR = "/Volumes/Shared/MasterThesis/RAG_Evaluation_Framework/data"
OUTPUT_PATH = "./output/document_summaries.json"

# --- 3. å·¥å…·ç¨‹å¼ ---
def load_json(path):
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_json(data, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def main():
    # è®€å–ç´¢å¼•èˆ‡æª”æ¡ˆå…§å®¹
    index_data = load_json(INDEX_PATH)
    file_map = {doc["file_name"].strip(): doc["doc_id"] for doc in index_data.get("documents", [])}

    # è®€å–ç¾æœ‰çµæœ (æ–·é»çºŒå‚³é—œéµ)
    final_report = load_json(OUTPUT_PATH)
    done_ids = {r["doc_id"] for r in final_report}
    print(f"ğŸ”„ åµæ¸¬åˆ°å·²å®Œæˆ {len(done_ids)} é¡Œï¼Œå°‡å¾ä¸‹ä¸€é¡Œé–‹å§‹...")

    # 1. è¼‰å…¥ä¸¦æº–å‚™å¿«å–å…§å®¹ (åƒ…åœ¨æœ‰æ–°é¡Œç›®è¦è·‘æ™‚æ‰åš)
    if len(done_ids) < len(file_map):
        print("ğŸ“‚ è¼‰å…¥æ–‡æª”ä¸¦å»ºç«‹å¿«å–...")
        langchain_docs = load_documents(DATA_DIR, clean=True)
        all_contents = []
        for doc in langchain_docs:
            f_name = os.path.basename(doc.metadata.get("source", "")).strip()
            d_id = file_map.get(f_name, "Unknown")
            all_contents.append(f"ã€ID: {d_id} | æª”å: {f_name}ã€‘\n{doc.page_content}\n---")

        cache = genai.caching.CachedContent.create(
            model=MODEL_NAME,
            display_name="ccu_rule_documents_cache",
            contents=all_contents,
            ttl=timedelta(hours=1)
        )

        model = genai.GenerativeModel.from_cached_content(
            cached_content=cache,
            generation_config={
                "temperature": 0,
                "response_mime_type": "application/json",
                "response_schema": SummaryResponse,
            }
        )

        # 2. æ‰¹æ¬¡è©¢å•è¿´åœˆ
        for doc_item in index_data.get("documents", []):
            doc_id = doc_item["doc_id"]
            doc_name = doc_item["file_name"]
            if doc_id in done_ids:
                continue

            print(f"ğŸ” æ­£åœ¨è™•ç† {doc_id} æ–‡æª”: {doc_name[:15]}...")
            prompt = f"æ–‡æª”åç¨±ï¼š{doc_name}\nä»»å‹™ï¼šè«‹é‡å°æ­¤æ–‡æª”å…§å®¹ç”Ÿæˆ 150-200 å­—çš„ã€æª¢ç´¢å°ˆç”¨æ‘˜è¦ã€ã€‚è«‹åŒ…å«ï¼š1.æ ¸å¿ƒè¦ç¯„äº‹é …ï¼ˆå¦‚ï¼šä¼‘å­¸ã€å­¸åˆ†æŠµå…ã€çå‹µç”³è«‹ï¼‰ã€‚2.é—œéµé™åˆ¶æ¢ä»¶ã€‚ç›®çš„æ˜¯è®“å¦ä¸€å€‹ LLM èƒ½åƒ…æ†‘æ­¤æ‘˜è¦åˆ¤æ–·è©²æ–‡æª”æ˜¯å¦èˆ‡ä½¿ç”¨è€…çš„å•é¡Œç›¸é—œã€‚"

            try:
                response = model.generate_content(prompt)
                res_json = json.loads(response.text)

                final_report.append({
                    "doc_id": doc_id,
                    "doc_name": doc_name,
                    "summary": res_json.get("summary", ""),
                    "keywords": res_json.get("keywords", [])
                })
                
                # æ¯ä»½æ–‡æª”è·‘å®Œç«‹å³å­˜æª”ï¼Œé˜²æ­¢ç¨‹å¼å´©æ½°
                save_json(final_report, OUTPUT_PATH)
                
            except Exception as e:
                print(f"âŒ {doc_id} å¤±æ•—: {e}")
                # é‡åˆ°éŒ¯èª¤é€šå¸¸æ˜¯ API é™åˆ¶ï¼Œç¨å¾®ä¼‘æ¯é•·ä¸€é»
                time.sleep(30)
                continue

            time.sleep(5) # æ­£å¸¸é–“éš”

        cache.delete()
        print(f"âœ… ä»»å‹™å®Œæˆï¼çµæœå„²å­˜æ–¼ {OUTPUT_PATH}")
    else:
        print("âœ¨ æ‰€æœ‰é¡Œç›®çš†å·²å®Œæˆã€‚")

if __name__ == "__main__":
    main()
