[Model]
; 原始 TAIDE 模型名稱
BASE_MODEL_NAME = taide/Llama-3.1-TAIDE-LX-8B-Chat
; 微調後的新模型名稱
NEW_MODEL_NAME = KCWen/taide
; 最大序列長度（LX 版本支援長文本）
MAX_SEQ_LENGTH = 4096

[LoRA]
; LoRA Rank（論文實驗可調整此參數：8, 16, 32）
R = 16
; LoRA Alpha
LORA_ALPHA = 16
; LoRA Dropout
LORA_DROPOUT = 0.0
; Bias 設定（none, all, lora_only）
BIAS = none
; 目標模組（用逗號分隔）
TARGET_MODULES = q_proj,k_proj,v_proj,o_proj

[Dataset]
; 訓練資料集檔案路徑（JSONL 格式）
DATASET_FILE = train_data.jsonl

[Training]
; 每個裝置的訓練批次大小
PER_DEVICE_TRAIN_BATCH_SIZE = 2
; 梯度累積步數
GRADIENT_ACCUMULATION_STEPS = 4
; 學習率
LEARNING_RATE = 2e-4
; 最大訓練步數（根據資料量調整，論文實驗建議跑 1-3 個 Epoch）
MAX_STEPS = 60
; Warmup 步數
WARMUP_STEPS = 5
; 輸出目錄
OUTPUT_DIR = outputs
; 日誌步數
LOGGING_STEPS = 1

[Quantization]
; 載入模型時是否使用 4-bit 量化（節省記憶體）
LOAD_IN_4BIT = true
; GGUF 量化方法（導出為 4-bit 量化 GGUF，適合 Intel Mac）
QUANTIZATION_METHOD = q4_k_m

[Hardware]
; 是否使用 FP16（自動檢測，如果支援 BF16 則使用 BF16）
AUTO_FP16_BF16 = true
